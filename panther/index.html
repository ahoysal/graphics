<html>

<head>
	<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=default'></script>
	<link rel="preconnect" href="https://fonts.googleapis.com">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link
		href="https://fonts.googleapis.com/css2?family=Montserrat:ital,wght@0,100..900;1,100..900&family=Press+Start+2P&display=swap"
		rel="stylesheet">
	<link rel="stylesheet" href="../style.css">
</head>

<body>
	<div class="container">
		<h1>Panther</h1>
		<div style="text-align: center;">(CS184 Summer 2025 Homework 3 Write-Up)</div><br>
		<div style="text-align: center;">~ Aagrim Hoysal ~</div>
		<div style="text-align: center;">
			<a href="https://ahoysal.github.io/graphics/panther">Loop Link</a> |
			<a href="https://github.com/cal-cs184/hw-pathtracer-updated-panther" target="_blank">GitHub Repo</a>
		</div>

		<!--
		We've already added one heading per part, to make your write-up as navigable when grading. Please fit your write-up within these sections!
		-->

		<figure>
			<img src="images/adaptive/banana.png" width="90%" title="Adaptive sampling (2.5% tolerance, max 8192 rays)
Global illumination (Russian roulette with 30% termination rate, max 100 bounces)
BVH (976 million rays traced, 8 million rays/second)">
			<figcaption>A render from the final pathtracer (122 seconds) [render settings in tooltip]</figcaption>
		</figure>

		<h2>Overview</h2>
		Panther is a path tracer that implements bounding volume hierarchies, light sampling, global illumination, and
		adaptive sampling. It uses probability extensively to make sure it converges to an accurate image as well as
		stays feasible to compute, utilizing methods such as importance-sampled Monte Carlo integration and Russian
		roulette.
		<br><br>
		Theoretically, I found the most interesting thing to be how much probability helps the render not only become
		resource efficient, but also how it is required to make the image visually correct. The fact that a way to get
		an unbiased estimator is to randomly sample instead of terminating after some amount of bounces surprised me,
		and in many different areas probability is critical to render anything. The ideas of importance sampling didn't
		seem to critical at first until I had to implement it <i>everywhere</i>. Now I see why UCSB requires computer
		science majors to take <a href="https://catalog.ucsb.edu/courses/PSTAT%20120A">Probability and Statistics</a>;
		it really did help with the conceptual underpinnings of this project.
		<br><br>
		Implementation wise, it was very satisfying to see everything being used in various ways: rays are just
		universally helpful, the BVH sped up everything astronomically, the BSDF and sampling functions could be used in
		different places.


		<h2>Ray Generation and Scene Intersection (Part 1)</h2>
		<h3>From Pixel to World: Generating Rays</h3>
		<figure style="float: inline-end;">
			<img src="images/spheresNormals.png" width="300px">
			<figcaption>Spheres rendered by normal</figcaption>
			<br>
			<img src="images/dragonNormals.png" width="300px">
			<figcaption>Dragon rendered by normal</figcaption>
			<br>
			<img src="images/bunnyNormals.png" width="300px">
			<figcaption>Bunny rendered by normal</figcaption>
		</figure>
		The basics of a path tracer are sending out rays and finding out what they hit. Thus, we need to find a way to
		generate rays from the camera in the direction of each pixel on the screen. Since we start with an \((x, y)\)
		coordinate in terms of pixels, we can make the rendering resolution agnostic by converting the pixel coordinate
		into a normalized image space with \((0, 0)\) being the bottom left of the image and \((1,1)\) being the top
		right by dividing by pixel by the screen resolution. By normalizing the image space, we can then convert from
		image space into camera space through shifting and scaling the \([0, 1)\) normalized image space range on both
		the x and y axes to \([-\tan(\frac{\text{fov}}{2}), \tan(\frac{\text{fov}}{2}))\), with a separate horizontal
		and vertical \(\text{fov}\) used on each axis. In addition, since image space is in 2D and camera space is in
		3D, we add a standard -1 for the Z axis, since by convention all points on the image plane in camera space are
		on \(z = -1\).
		<br><br>
		This coordinate directly gives us the direction of the ray, since it is at the location of the pixel coordinate
		on the image in camera space and the camera (and by extension the origin of the ray) is at \(0, 0\) in camera
		space. The vector from the camera to our pixel's transformed coordinates is then the transformed value minus the
		origin, which is just the transformed value. Finally, because we know the ray originates at the camera, all we
		need to do to convert from the ray in camera space to world space is simply apply the rotation matrix of the
		camera to rotate the ray in conjunction with the camera. Since vectors (as opposed to points) aren't affected by
		translations, we don't need to worry about translations or homogenous coordinates, and we can just apply the
		rotation matrix. The transformed vector now gives us a vector pointing in the direction from the camera origin
		to the pixel's coordinates on the image plane in world space. This vector normalized, along with the camera's
		position in world space serving as the ray's origin, defines a ray going through a specified pixel in world
		space. We can also add some randomness by peturbing the ray by a random amount between \([0,1)\)in the \(x\) and
		\(y\) directions while in pixel space to sample different directions within the same pixel.
		<br><br>
		<h3>Yielding for triangles</h3>
		<figure style="float: inline-end;">
			<img src="images/yield.png" width="200px">
			<figcaption>Rays must yield to triangles!</figcaption>
		</figure>
		If we can find where rays and objects intersect, we can figure out what rays hit. For simplicity, I only
		implemented triangle-ray intersections and sphere-ray intersections. Triangle ray intersections are easily found
		by using the barycentric definition of a plane by defining all points on a plane as the weighted average of the
		three vertices of a triangle, where all weights add up to one (see <a
			href="https://ahoysal.github.io/cs184/rasterization/#part4">earlier explanation of barycentric
			coordinates</a>). By substituting the equation for all points on a ray (defined as \(O_{rigin} +
		D_{irection}t\) for \(t \ge 0\)) as a point, we can solve for the ray and plane's intersection.
		\[
		O - v_0 = \beta (v_1 - v_0) + \gamma (v_2 - v_0) - Dt
		\equiv
		\begin{cases}
		p = O + Dt & \text{Ray equation} \\
		p = (1-\beta-\gamma) v_0 + \beta v_1 + \gamma v_2 & \text{Barycentric plane}
		\end{cases}
		\]
		This lends itself to an elegant matrix:
		\[
		\begin{bmatrix}
		-D & v_1-v_0 & v_2-v_0
		\end{bmatrix}
		\begin{bmatrix}
		t \\ \beta \\ \gamma
		\end{bmatrix}
		=
		O - v_0
		\]
		We can <a href="https://cs184.eecs.berkeley.edu/su25/assets/discussions/06-sol.pdf">solve for \(t, \beta,
			\gamma\) using Cramer's rule</a>, giving us the intersection point on the triangle's plane in barycentric
		coordinates. Then, by checking if the values of \(\alpha (= 1 - \beta - \gamma), \beta, \gamma \ge 0\), we can
		tell if the intersection is within the triangle or not. This process is called the Möller-Trumbore intersection
		algorithm.
		<br><br>
		By using Möller-Trumbore, we get some intersection data for free. The \(t\) value serves as distance along the
		ray, and so lets us cut values too close or far from the camera and prevent intersecting occluded objects by
		only taking the closest distance. The barycentric coordinates can be used to interpolate between assigned normal
		vectors for each vertex on the triangle to give a normal at the intersection point as well.
		<h3>Stopping when Spheres are Near</h3>
		Sphere intersections follow the same basic method, but instead substituting the ray equation into an implicitly
		defined sphere. Defining the sphere's center as \(C\),
		\[
		\begin{cases}
		P = O + Dt & \text{Ray equation} \\
		r^2 = (x-C_x)^2 + (y-C_y)^2 + (z-C_z)^2 = \sum_{i = 1}^{3} (P[i] - C[i])^2 & \text{Implicit Sphere}
		\end{cases}
		\]
		This can be formed into a quadratic equation:
		\[
		0 = -r^2 + \sum_{i = 1}^{3} (O[i] + D[i]t - C[i])^2
		\equiv
		0 = (O^2 - 2O\cdot C + C^2 - r^2) + 2D\cdot(O-C)t + D^2t^2
		\]

		<figure style="float: inline-end;">
			<img src="images/yields.png" width="200px">
			<figcaption>I don't think the joke<br>works in this context...</figcaption>
		</figure>
		Which gives us zero roots (no intersection), one root (ray is tangent), or two roots (ray passing through the
		sphere). We can then compare \(t\) values in the same way as we did for triangles to only check the nearest
		intersections. Spheres also give the ability to compute perfect normals by computing the normalized vector from
		the sphere's origin to the intersection point (which we can find by plugging in the computed \(t\) value into
		the original ray equation).

		<h2>Bounding Volume Hierarchy (Part 2)</h2>

		A Bounding Volume Hierarchy is an acceleration structure designed to cut down on the number of intersection
		tests needed per ray. It achieves this by creating a binary tree of progressively smaller bounding boxes around
		other bounding boxes or primitives until less than a certain number of primitives remain within, which becomes a
		leaf of the tree. If a ray doesn't intersect the parent's bounding box, all children's bounding boxes (and as a
		result all of the leaf nodes' primitives) are contained within and so the tree can be pruned. This leads to
		O(log n) as opposed to O(n) intersection tests in the average case where n is the number of triangles, since the
		tree's height is O(log n) and the number of primitives within each leaf is bounded by a constant "max primitives
		per node".
		<br><br>
		To construct the BVH, we can run a recursive algorithm to build nodes. We first find the minimum bounding box of
		all primitives this node is responsible for. If the number of primitives is less than a constant max amount of
		primitives per node, then this node can be a leaf and we don't need to do any more splitting. Otherwise, we'll
		split the primitives along the longest axis. There are many different heuristics for how to split the primitives
		into two smaller sets, but I chose to split through the median of the primitive's centroid's coordinate along
		the longest axis. This just meant sorting the primitive centroids by their longest axis's coordinate (in place,
		see <a href="ec">extra credit section</a>) and assigning the low half to one child and the high half to another
		child. These two
		assignments can then be delegated to a recursive call of the same algorithm. Each child now has half the
		elements of the parent because we split by the median, so eventually there will be fewer than the max amount of
		primitives per node and the recursion will terminate.
		<br><br>
		The BVH greatly sped up rendering times, as seen by the following 8 thread, 600x800 renders. For Max Planck
		(50801 triangles), rendering time was cut from 96.7 seconds to 0.150 seconds (0.0845 for creating the BVH and
		0.0651 to find ray intersections), going from processing five thousand to 5.97 million rays/second. For the
		angel statue, 236 seconds became 0.292 seconds (0.233s BVH + 0.0597s intersections), with 6.74 million
		rays/second instead of 2k. The intersections per ray dropped especially fast because the scenes contained
		significant areas of empty space, where many rays were immediately able to exit early because they didn't hit
		the first bounding box. In scenes with less void, the effect might be less dramatic but still significant.
		Bounding volume hierarchies also let me render a version of the angel statue in 1.10 seconds with 16 rays per
		pixel, which went so impossibly slow on the non-BVH version that I aborted it after a few minutes at single
		digit percentage completion.
		<br><br>
		* Sidenote, I didn't include intersections per ray as a metric because I think race conditions were messing with
		the values since the count was highly dependent on the number of threads used. I didn't want to influence
		timing by adding mutexes, and I didn't want to benchmark on one thread because the non-BVH code would run very
		slow. Rays/second did also vary, but were still on the same order of magnitude as their single threaded
		counterparts.
		<br><br>

		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: separate; border-width: 1em;">
				<tr>
					<td>Model (Primitive Count)</td>
					<td>Normal Render</td>
					<td>Non-BVH Stats</td>
					<td>BVH Stats<br>(BVH construction time + render time)</td>
				</tr>
				<tr>
					<td>Beast (64618)</td>
					<td style="text-align: center;">
						<img src="images/beast.png" width="200px" />
					</td>
					<td>78.3s<br>6,100 rays/second</td>
					<td>0.150s (0.0845s + 0.0651s)<br>6,370,000 rays/second</td>
				</tr>
				<tr>
					<td>Max Planck (50801)</td>
					<td style="text-align: center;">
						<img src="images/maxplank.png" width="200px" />
					</td>
					<td>96.7s<br>5,000 rays/second</td>
					<td>0.150s (0.100s + 0.0498s)<br>5,970,000 rays/second</td>
				</tr>
				<tr>
					<td>Angel (133796)<br>1 ray/pixel</td>
					<td style="text-align: center;">
						<img src="images/lucy.png" width="200px" />
					</td>
					<td>236s<br>2,000 rays/second</td>
					<td>0.292s (0.233s + 0.0597s)<br>6,740,000 rays/second</td>
				</tr>
				<tr>
					<td>Angel (133796)<br>16 rays/pixel</td>
					<td style="text-align: center;">
						<img src="images/lucy1.png" width="200px" />
					</td>
					<td>N/A</td>
					<td>1.10s (0.232s + 0.872s)<br>6,390,000 rays/second</td>
				</tr>
			</table>
		</div>

		<h2>Direct Illumination (Part 3)</h2>
		<blockquote><strong>For CS184 graders!</strong> Most of this is context leading into the actual write-up
			questions. The actual answers are under the heading “Monte Carlo (finally some fun!)".</blockquote>
		Direct illumination is the irradiance that a pathtracer receives from a point, only based on rays that go
		directly to a light source or bounce off one surface and then hit a light source. As you can imagine, this is
		simpler than considering rays that take more than one bounce, but does require thinking about how to get the
		radiance from the one bounce case. We need to integrate incoming light and its radiance (even if it's zero) over
		the hemisphere to figure out how much gets reflected, and the practical way to do this is through Monte Carlo
		integration, which requires sampling the hemisphere.
		<br><br>
		Lets worry about <i>how</i> we sample later. Monte Carlo integration uses the rendering equation

		\[
		L_o(p, \omega_o) = L_e(p, \omega_o) + \int_{\mathcal{H}^2} L_i(p, \omega_i) \cdot f_r(p, \omega_i \to \omega_o)
		\cdot \cos \theta_i \cdot d\omega_i
		\]

		Integrating over the hemisphere \(\mathcal{H}^2\) for all incoming solid angles \(\omega_i\) where \(L_o\) is
		the light out, \(L_e\) is the light emitted by the surface, \(f_r\) is how light scatters/gets reflected based
		on the solid angles of the “viewing"* solid angle (\(\omega_o\)), the light coming from the solid angle in
		\(L_i\), and the cosine of the incoming solid angle with the normal of the surface. The derivation is out of
		scope for this explanation, but the integral can be interpreted as adding up all the incoming light rays and
		multiplying by how much of that light gets reflected in the outgoing direction \(\omega_o\) that we're
		interested in (the cosine term comes from <a
			href="https://en.wikipedia.org/wiki/Lambert%27s_cosine_law">Lambert's cosine law</a>). The two questions
		that we will tackle are defining (narrowly for diffuse surfaces only) \(f_r\), the function that returns the
		amount of light that reflects in a direction based on the incoming and outgoing rays, and how to approximate
		this integral with the aforementioned Monte Carlo method.
		<br><br>
		\(f_r\) is thankfully quite simple in the case of diffuse Lambertian surfaces (things that tend to look matte);
		any light ray coming in gets scattered in all directions of the hemisphere equally. Thus our diffuse \(f_r\) is
		not a function of the incoming or outgoing ray at all and only needs to take into account how much light gets
		absorbed by the surface, what we call its albedo. The function ends up being \(f_r = \frac{\rho}{\pi}\) with
		\(\rho\) being the albedo, divided by a \(\pi\) term as a result of the energy being redistributed across the
		hemisphere.
		<br><br>
		Now that we have a feel for the parts of the integral, we actually need to evaluate it. Its finally time to get
		into Monte Carlo integration.

		<h3>Monte Carlo (finally some fun!)</h3>
		<figure style="float: inline-end;">
			<figcaption><strong>Why importance sampled Monte Carlo?</strong></figcaption>
			<br>
			<img width="300px" src="images/ballsUHS.png">
			<figcaption>Doesn't use Monte Carlo<br>importance sampling (13.7s)</figcaption>
			<br>
			<img width="300px" src="images/ballsLS.png">
			<figcaption>Utilizes Monte Carlo<br>importance sampling (12.8s)</figcaption>
		</figure>
		The intuition for Monte Carlo integration is generally that we will take random samples of the function within
		the integration region. Each of these random samples will be weighted by the probability (in a loose sense, we
		eventually use the PDF) that that specific sample was chosen, so that samples in neighborhoods we sample less
		can represent that neighborhood proportionally more to make up for the lack of other samples. The probability
		weighting, which is technically <i>importance-sampled</i> Monte Carlo, may seem like an unnecessary
		overcomplication, but it helps improve converging speed by sampling fewer rays with near-zero contribution.
		<br><br>
		Formally, this version of Monte Carlo looks like, with some \(x_i\) sampled from a non-zero probability
		distribution over \([a, b)\) with pdf \(p(x_i)\) and \(N\) samples,

		\[
		\int_{a}^{b} f(x) dx = \frac{1}{N}\sum_{i=1}^N \frac{f(x_i)}{p(x_i)}
		\]

		In combination with our rendering equation, we can substitute the integral with the importance sampled Monte
		Carlo. We can also replace \(L_i(p, \omega_i)\) with \(L_e(p_{next}, \omega_i)\), the light from the second ray
		sent, since we are doing direct illumination and don't care about light that wasn't emitted from the first
		surface in that direction specifically. Thus we get (with \(\cos \theta_i\) being replaced by the dot product of
		the two unit vectors \(\omega_i\) and \(n\), the normal),

		\[
		L_o(p, \omega_o) = L_e(p, \omega_o) + \frac{1}{N} \sum_{i=1}^N \frac{L_e(p_{next}, x_i) \cdot f_r(p, x_i \to
		\omega_o) \cdot (x_i \cdot n)}{p(x_i)}
		\]

		I implemented two sampling methods– uniform hemisphere sampling and light sampling. Uniform hemisphere sampling
		does exactly what it says on the tin, i.e. sampling each direction on the hemisphere with equal probability. The
		probability density function is uniform over the integration region, meaning we don't converge any faster than
		normal Monte Carlo without importance sampling. The PDF \(p(x_i)\) is always \(\frac{1}{2\pi}\). It simply is
		just sending random rays from every initial bounce in a constant amount of directions to see if it hits a light
		(and polling how much it is emitting) or not. My code literally loops \(N\) times, samples a uniform random ray,
		finds \(f_r\), finds the random ray's next intersection, and computes the formula above. The only tricky part
		was morphing from world to local coordinate spaces and back, as well as making sure to negate the
		incoming/outgoing rays because the pathtracer propagates paths backwards. This method gave results, but were
		much grainer for the same amount of samples compared to the next approach.
		<br><br>
		<figure style="float: inline-end;">
			<img width="300px" src="images/dragon.png">
			<figcaption>Dragon rendered with light sampling.<br>Uniform hemisphere doesn't work with<br>point lights,
				so can't compare.</figcaption>
		</figure>
		Light sampling actually takes advantage of the importance-sampled Monte Carlo method. We modify the PDF to scale
		with the predicted distribution of light contribution, which can be done through a higher PDF in the direction
		of the light and its surrounding areas in the case of an area light. We can then sample proportionally more in
		that region while still acknowledging the rest of the hemisphere with fewer, more weighted samples. In practice,
		there are some implementation details to take care of. It tends to be best to send a set amount of rays around
		<i>each</i> light. For point lights, we sample once and extrapolate that one sample for all intended samples of
		the light. We also need to take care of obstructions, since our sample direction that tends to face the light
		doesn't check for occlusion. This can be fixed by sending a ray from the surface to the light source and
		checking for collisions; if there is one, then the light source is blocked and its contribution is zero.
		Otherwise, it's calculating the same values as the uniform sample, except the PDF varies.
		<br><br>
		Light sampling lets soft shadows form with much less computation than uniform sampling, although it still
		requires a moderate amount of rays to converge. The table below provides renderings at one ray per pixel, but
		with different amounts of rays sent after the first bounce with light sampling. As you can see, more rays form
		soft shadows with less noise. (All renders at one sample per pixel)
		<br>
		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: separate; border-spacing: 1em; ">
				<tr>
					<td style="text-align: center;">
						<img src="images/bunnyLS1.png" width="300px" />
						<figcaption>Bunny at 1 ray/light.<br>Very noisy shadows.</figcaption>
					</td>
					<td style="text-align: center;">
						<img src="images/bunnyLS4.png" width="300px" />
						<figcaption>Bunny at 4 rays/light.<br>Noisy shadows.</figcaption>
					</td>
				</tr>
				<tr>
					<td style="text-align: center;">
						<img src="images/bunnyLS16.png" width="300px" />
						<figcaption>Bunny at 16 rays/light.<br>Less noisy shadows.</figcaption>
					</td>
					<td style="text-align: center;">
						<img src="images/bunnyLS64.png" width="300px" />
						<figcaption>Bunny at 64 rays/light.<br>Better shadows.</figcaption>
					</td>
				</tr>
			</table>
		</div>

		<figure style="float: inline-end;">
			<img width="200px" src="images/bunnyH8-64.png">
			<figcaption>Uniform Hemisphere (9.53s)<br>8 rays/pixel, 64 rays/bounce</figcaption>
			<img width="200px" src="images/bunnyL2-2.png">
			<figcaption>Light Sampling (0.0960s)<br>2 rays/pixel, 2 rays/bounce</figcaption>
		</figure>
		<br>
		Uniform hemisphere sampling and lighting sampling will both eventually converge to the same rendered image– the
		difference is how fast. Uniform sampling, with the same amount of rays, will waste more rays on areas with no
		contribution to the final illuminance and a smaller proportion of the rays will hit the light. This means that
		information about direct lights propagates slower over the number of samples taken. Meanwhile, light sampling
		allows for information about the light to propagate quicker as the number of samples grows by tending those
		samples towards lights. The computational efficiency difference is stark; compare the two similar images to the
		right. The image quality is similar, with less noise on average in the light sampling image but better shadows
		in the uniform hemisphere sampling image. However, despite being of similar quality, the uniform hemisphere
		sampling image took 9.53 seconds to render, with 8 samples per pixel and 64 rays for the second bounce. The
		light sampling image only took 0.0960 seconds with a piddly 2 samples per pixel and 2 rays for the second
		bounce, within spitting distance of real time rendering. If we allocate the same amount of resources as the
		uniform hemisphere sample (8 samples per pixel, 64 rays for the second bounce), light sampling gives us the
		bottom image in 8.97 seconds.
		<figure>
			<img src="images/bunnyL8-64.png">
			<figcaption>Light Sampling (8.97s)<br>8 rays/pixel, 64 rays/bounce</figcaption>
		</figure>


		<h2>Global Illumination (Part 4)</h2>
		<figure style="float: inline-end;">
			<img src="images/globalIll/spheresA.png" width="200px">
			<figcaption>Spheres with<br>global illumination!</figcaption>
			<br>
			<img src="images/globalIll/walle.png" width="200px">
			<figcaption>Wall-e, look at those<br>nice shadows!</figcaption>
		</figure>
		Sometimes (most of the time) we want to simulate more than just one bounce. To do this, we add <strong>indirect
			lighting</strong> to the previously computed direct lighting. Indirect lighting considers different paths
		with multiple bounces the light could take to the camera, including bounces off of other objects. Remember how
		we substituted \(L_i(p, \omega_i)\) with \(L_e(p_{next}, \omega_i)\) for direct lighting, since we only
		considered emission after one bounce? We now need to actually approximate \(L_i(p, \omega_i)\). Intuitively this
		means we need to calculate the next “reverse bounce”, which we can do by sending another ray out in a random
		direction, tracing it to its next intersection with geometry, and finding the light out from that surface
		through recursively calling the rendering equation again.
		<br><br>
		Implementation-wise, I started with an “radiance from at least one bounce” function, which runs the direct
		lighting algorithm for the bounce, as well as selecting a random uniform hemisphere direction (based on the even
		dispersal of light from the diffuse BSDF) to trace and call the same function again to get the light from the
		second bounce. The next sampled light then gets multiplied and normalized by the rendering equation and is added
		to the direct lighting, which is then returned as the radiance coming from that point.
		<br><br>
		We can split our render under this framework further into the contributions to the radiance from a specific
		number of bounces (i.e, after a certain number of recursive calls). Below is a few images showing those
		contributions, per number of bounces between the light source and the camera.
		<br><br>
		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: separate; border-spacing: 0.5em; ">
				<tr>
					<td>
						<strong>Contribution from<br>N-bounce rays only</strong>
					</td>
					<td>
						<strong>Cumulative contribution from<br>N-bounce or fewer rays</strong>
					</td>
				</tr>
				<tr>
					<td>
						<img src="images/globalIll/single0.png" width="300px">
						<figcaption>Zero bounce (source only)</figcaption>
					</td>
					<td>
						<img src="images/globalIll/det0.png" width="300px">
						<figcaption>Zero or fewer bounces</figcaption>
					</td>
				</tr>
				<tr>
					<td>
						<img src="images/globalIll/single1.png" width="300px">
						<figcaption>One bounce only</figcaption>
					</td>
					<td>
						<img src="images/globalIll/det1.png" width="300px">
						<figcaption>One or fewer bounces</figcaption>
					</td>
				</tr>
				<tr>
					<td>
						<img src="images/globalIll/single2.png" width="300px">
						<figcaption>Two bounces only</figcaption>
					</td>
					<td>
						<img src="images/globalIll/det2.png" width="300px">
						<figcaption>Two or fewer bounces</figcaption>
					</td>
				</tr>
				<tr>
					<td>
						<img src="images/globalIll/single3.png" width="300px">
						<figcaption>Three bounces only</figcaption>
					</td>
					<td>
						<img src="images/globalIll/det3.png" width="300px">
						<figcaption>Three or fewer bounces</figcaption>
					</td>
				</tr>
				<tr>
					<td>
						<img src="images/globalIll/single4.png" width="300px">
						<figcaption>Four bounces only</figcaption>
					</td>
					<td>
						<img src="images/globalIll/det4.png" width="300px">
						<figcaption>Four or fewer bounces</figcaption>
					</td>
				</tr>
				<tr>
					<td>
						<img src="images/globalIll/single5.png" width="300px">
						<figcaption>Five bounces only</figcaption>
					</td>
					<td>
						<img src="images/globalIll/det5.png" width="300px">
						<figcaption>Five or fewer bounces</figcaption>
					</td>
				</tr>
			</table>
		</div>
		<br>
		As you can see, the contributions progressively reduce. The second bounce tends to show light bouncing from
		the places where the light landed, typically near the floor, back up towards the roof, and we observe
		“underglow” on the bottom of objects. At this point, the roof gets lighting from light reflecting back onto it,
		whereas from the first two bounces the roof is pitch black. The third bounce generally moves into ambient light,
		providing light to shadows so they aren't pitch black and are more realistic. In general, raytracing as opposed
		to rasterization allows for secondary reflections to show up in the final image. This generates ambient light
		automatically that takes into account surrounding objects without a need for a faked “ambient light” constant.
		<br><br>
		We need to stop this recursion at some point. One way to do this is to call it a day once a ray has bounced some
		constant maximum amount of times. However, this won't converge to the final image as you increase the number of
		rays without increasing the number of bounces– we will never sample paths from the light that take more than the
		maximum amount of bounces to reach the camera. Therefore, we can use a similar trick to Monte Carlo integration
		and have some probability of stopping our recursion, and weight the contribution inverse proportionally to how
		likely it was to happen. For example, if there's a flat 70% chance we continue, we weight each result by
		\(\frac{1}{.7} \approx 1.43\) to let it proportionally represent the 30% that got terminated. This method is
		called Russian Roulette for… obvious reasons. We also keep the maximum bounce depth just in case, but set it to
		a high value as hopefully the contribution by that point is near zero.
		<br><br>
		Here's a comparison of images with direct (zero or one bounces) and indirect (more than two bounces, ended by
		Russian Roulette) illumination separately.
		<br><br>
		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: separate; border-spacing: 0.5em; ">
				<tr>
					<td>
						<img src="images/globalIll/directBunny.png">
						<figcaption>Bunny, direct illumination contribution<br>Notice the unnatural lack of color
							bleeding</figcaption>
					</td>
					<td>
						<img src="images/globalIll/indirectBunny.png">
						<figcaption>Bunny, indirect illumination contribution<br>Notice the lighting on the roof and
							ambient lighting</figcaption>
					</td>
				</tr>
				<tr>
					<td>
						<img src="images/globalIll/directBuilding.png">
						<figcaption>Building, direct illumination contribution</figcaption>
					</td>
					<td>
						<img src="images/globalIll/indirectBuilding.png">
						<figcaption>Building, indirect illumination contribution<br>Notice the outside gets no indirect
							lighting,<br>but the inside has light bouncing around between struts</figcaption>
					</td>
				</tr>
			</table>
		</div>
		<br>
		We can also see the effect of clamping the Russian Roulette at a certain amount of bounces. Compare the max ray
		depths below, with 100 being a stand-in for letting Russian Roulette kill \((1-(.7)^{100}) \approx 100\%\) of
		rays.
		<br><br>
		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: separate; border-spacing: 0.5em; ">
				<tr>
					<td>
						<img src="images/globalIll/rr0.png" width="300px">
						<figcaption>Max ray depth of zero<br>(source only)</figcaption>
					</td>
					<td>
						<img src="images/globalIll/rr1.png" width="300px">
						<figcaption>Max ray depth of one<br>(direct lighting)</figcaption>
					</td>
					<td>
						<img src="images/globalIll/rr2.png" width="300px">
						<figcaption>Max ray depth of two<br>(.7 survive rate)</figcaption>
					</td>
				</tr>
				<tr>
					<td>
						<img src="images/globalIll/rr3.png" width="300px">
						<figcaption>Max ray depth of three<br>(.7 survive rate)</figcaption>
					</td>
					<td>
						<img src="images/globalIll/rr4.png" width="300px">
						<figcaption>Max ray depth of four<br>(.7 survive rate)</figcaption>
					</td>
					<td>
						<img src="images/globalIll/rr100.png" width="300px">
						<figcaption>Max ray depth of 100<br>(.7 survive rate)</figcaption>
					</td>
				</tr>
			</table>
		</div>
		<br>
		I also rendered a banana with a different amount of rays per pixel. It gets progressively cleaner/less noisy as
		the number of rays increases.
		<br><br>
		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: separate; border-spacing: 0.5em; ">
				<tr>
					<td>
						<img src="images/globalIll/banana1.png" width="300px">
						<figcaption>One ray/pixel</figcaption>
					</td>
					<td>
						<img src="images/globalIll/banana2.png" width="300px">
						<figcaption>Two rays/pixel</figcaption>
					</td>
				</tr>
				<tr>
					<td>
						<img src="images/globalIll/banana4.png" width="300px">
						<figcaption>Four rays/pixel</figcaption>
					</td>
					<td>
						<img src="images/globalIll/banana8.png" width="300px">
						<figcaption>Eight rays/pixel</figcaption>
					</td>
				</tr>
				<tr>
					<td>
						<img src="images/globalIll/banana16.png" width="300px">
						<figcaption>Sixteen rays/pixel</figcaption>
					</td>
					<td>
						<img src="images/globalIll/banana64.png" width="300px">
						<figcaption>Sixty-four rays/pixel</figcaption>
					</td>
				</tr>
			</table>
		</div>
		<figure>
			<img src="images/globalIll/banana1024.png" width="600px">
			<figcaption>1024 rays/pixel, thats a lot of<span
					style='font-family: "Press Start 2P", monospace; font-weight: 200; font-style: normal; font-size: 0.5em;'>
					* Potassium</span></figcaption>
		</figure>

		<h2>Adaptive Sampling (Part 5)</h2>
		Adaptive sampling is simple at a high level; send out more rays proportionally to how different the samples are
		from each other. We take in a constant tolerance (ex. we want the illuminance to be within 5% of the true value)
		and for each pixel perform a basic 95% confidence interval test with the mean and variance of the illuminance of
		the samples. If we're 95% confident that the mean lies within our constant tolerance, we can stop sending out
		more rays, as we are close
		enough. All this confidence interval-ing can be expressed by checking the following inequality:
		\[
		1.96 \frac{\sigma}{\sqrt{n}} \ge T \cdot \mu
		\]
		Where \(T\) is the tolerance. Once this condition is satisfied, we deem the pixel “converged enough” and stop
		sampling more. This allows for more complex regions to be sampled more than simpler regions, and increases the
		fidelity without increasing runtime by redistributing rays from simpler pixels to ones that need more rays to
		get rid of noise.
		<br><br>
		There are a few tricks to make the code slightly faster. One is batching, i.e checking the condition every \(k\)
		rays versus every ray, just because the convergence check involves slower operations like a square root and
		division. Another optimization is using an alternate formula for variance using the first and second moments:
		\[
		\sigma^2 = E[X^2] - E[X] = \frac{\sum (x^2)}{n} - \frac{(\sum x)^2}{n^2} = \frac{(\sum (x^2)) - \frac{(\sum
		x)^2}{n}}{n}
		\]
		Thus, we can just accumulate \(\sum (x^2)\) and \(\sum x\) and plug into the formula to get the variance at any
		point while sampling without having to reiterate over all the points. This is also useful since we need \(\sum
		x\) anyways to calculate the mean. The last thing that needs to be done is to loop sampling while the
		convergence term is less than the tolerance times the mean and the samples are less than the maximum samples
		(just in case the illuminance doesn't converge in a reasonable amount of samples).
		<br>
		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: separate; border-spacing: 0.5em; ">
				<tr>
					<td>
						<img src="images/adaptive/dragon.png" width="400px">
						<figcaption>Rendered dragon with adaptive sampling (65.1s)</figcaption>
					</td>
					<td>
						<img src="images/adaptive/dragon_rate.png" width="400px">
						<figcaption>Dragon sampling rate (red > green > blue)</figcaption>
					</td>
				</tr>
				<tr>
					<td>
						<img src="images/adaptive/spheresAS.png" width="400px">
						<figcaption>Rendered spheres with adaptive sampling (76.4s)</figcaption>
					</td>
					<td>
						<img src="images/adaptive/spheresAS_rate.png" width="400px">
						<figcaption>Sphere sampling rate (red > green > blue)</figcaption>
					</td>
				</tr>
			</table>
		</div>
		<br>
		Notice how edges and shadows tend to need more rays (check out that clear increase in the dragon's shadow!) and
		any surfaces that face other surfaces (such as the ground) and therefore might get more indirect illumination.
		Both images were rendered with Russian Roulette, 100 bounces max, 1 light sample, and adaptive sampling with
		2048 rays max, a batch size of 64 rays, and a tolerance threshold of 5%.

		<h2 id="ec">Extra Credit (Part 6)</h2>
		<h3>Memory efficient BVH</h3>
		I implemented a more memory efficient BVH by having only one array, sorting along one axis in place using the
		<code lang="c++">std::sort</code> function, and then passing the [start, middle) and [middle, end) iterators to
		the children nodes of the BVH. Thus starting from the original array, they are sorted along the median pivot on
		one axis, then within the left and right side are again split along the median along another axis. Thus the root
		node has descendants containing indexes \([0, n/2)\) and \([n/2, n)\), the right child has descendants \([n/2,
		3n/4)\) and \([3n/4, n)\), etc. In general, a node of depth \(d\) contains descendants \([\frac{k}{2^d},
		\frac{k+1}{2^d})\) for some value \(k\). Since these are all contiguous in the vector and sorted in place, the
		construction of the BVH takes up no auxiliary space. The naïve approach requires two times the persistent memory
		to store all the new vectors, and while constructing the BVH three times the original memory due to recursively
		cloning half the list.

		<h3>Halton (quasi-random) per pixel sampling</h3>
		Halton points, as discussed <a href="../rasterization/#halton">in my rasterization write-up</a>, allows for
		quasai-random distribution of directions in which the ray points to on the pixel. This mitigates risks of
		sampling, by random chance, a partiuclar part of the pixel, for example most of the samples ending up on the
		right side. Halton points guarantee that there are an equal distribution of points in every range as it goes to
		infinity.
		<br><br>
		We can use the same Halton sampling within a unit square as used in the rasterization write-up to figure out
		where within the pixel we want the ray to go through. I just replaced the grid sampler that randomly samples a
		value from \([0,1) x [0,1)\) to Halton points generated from the coprimes 2 and 3. This provides more assurance
		that our quasi-random samples will always represent the grid well, even at low numbers of samples. For
		efficiency reasons, I computed the Halton points in a static lookup vector that stores all previous values, and
		only compute new ones when the next Halton point hasn't already been computed.
		<br>
		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: separate; border-spacing: 0.5em; ">
				<tr>
					<td>
						<img src="images/lucyUniform.png" width="400px">
						<figcaption>Lucy rendered with uniform random sampling<br>4 rays/pixel</figcaption>
					</td>
					<td>
						<img src="images/lucyHalton.png" width="400px">
						<figcaption>Lucy rendered with Halton point sampling<br>4 rays/pixel</figcaption>
					</td>
				</tr>
			</table>
			<table style="width: 100%; text-align: center; border-collapse: separate; border-spacing: 0.5em; ">
				<tr>
					<td>
						<img src="images/lucyDif.png" width="400px">
						<figcaption>Difference of the above two images</figcaption>
					</td>
					<td>
						<img src="images/lucyDif2.png" width="400px">
						<figcaption>Insert. Green is uniform random, blue is Halton points</figcaption>
					</td>
				</tr>
			</table>
		</div>
		<br>
		Notice how the difference image has large differences along the edges of the statue. In those areas, you can see
		the edge of the wing is much more well defined in the Halton point sampling image, even with only four samples
		per pixel. This is because they are more evenly distrubuted, without being super predictable like a grid would
		be in order to not cause common aliasing.

		<h2>(Un)acknowelgement of AI</h2>
		I did not use AI for this assignment, aside from
		<ul>
			<li>Glancing over the Google AI overview to confirm my understanding of some concepts/formulas and to find
				actual sources with the webpages the overview links</li>
			<li>Spellcheck and grammar by typing out the content for this write-up in a Google Doc with Grammarly
				enabled</li>
			<li>Double checking \(\mu = \frac{s_1}{n}\sigma^2 = \frac{1}{n-1} \cdot (s_2 - \frac{s_1^2}{n})\) given in
				the project assignment webpage, which I'm still not conviced is right (I ended up rederiving the formula
				for variance anyways and not using that)
			</li>
		</ul>
	</div>


	<footer>
		Webpage based off <a href="https://github.com/cal-cs184/hw-webpage" target="_blank">CS 184 homework write up
			template</a>
	</footer>

</body>

</html>